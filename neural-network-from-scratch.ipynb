{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2984728,"sourceType":"datasetVersion","datasetId":1829286}],"dockerImageVersionId":30301,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"table-of-contents\" style=\"background-color:#000000; padding: 20px; margin: 10px; font-size: 110%; border-radius: 25px; box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\">\n  <h1 style=\"color:#00F1FF;\">TOC</h1>\n  <ol>\n    <li><a href=\"#1\" style=\"color: #00F1FF;\">1. Overview</a></li>\n     <li><a href=\"#2\" style=\"color: #00F1FF;\">2. Imports</a></li>\n    <li><a href=\"#3\" style=\"color: #00F1FF;\">3. Data Analysis</a></li>\n    <li><a href=\"#4\" style=\"color: #00F1FF;\">4. Data Preprocessing</a></li>\n    <li><a href=\"#5\" style=\"color: #00F1FF;\">5. Model Implementation Helper Functions </a></li>\n    <li><a href=\"#6\" style=\"color: #00F1FF;\">6. Model Implementation</a></li>\n    <li><a href=\"#7\" style=\"color: #00F1FF;\">7. Evaluation</a></li>\n    <li><a href=\"#8\" style=\"color: #00F1FF;\">8. Thank You</a></li>  \n  </ol>\n</div>\n\n<a id=\"1\"></a>\n<h1 style='background:#000000;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #00F1FF;'>Overview</center></h1>\n\n# Overview\n  \n\n    \n**Neural networks are often regarded as opaque, or \"black boxes\", due to their complex and abstract nature. However, many individuals, including myself, desire a deeper understanding of how they operate. in this notebook we will delve into the implementation and optimization of neural networks from scratch.**\n\n**Our aim is to explore the underlying principles and mechanisms of neural networks, and develop a practical understanding of their inner workings. We will begin by implementing a basic neural network, utilizing fundamental concepts and techniques.**\n\n**With this approach, we can gain a more profound understanding of neural networks, and develop the ability to tailor them to specific tasks and objectives. So, let us begin this journey of discovery and learning.**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h1 style='background:#000000;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #00F1FF;'>Imports</center></h1>\n\n# Imports\n  ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:47.466Z","iopub.execute_input":"2023-02-04T11:18:47.46655Z","iopub.status.idle":"2023-02-04T11:18:47.474905Z","shell.execute_reply.started":"2023-02-04T11:18:47.466504Z","shell.execute_reply":"2023-02-04T11:18:47.473451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h1 style='background:#000000;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #00F1FF;'>Data Analysis</center></h1>\n\n# Data Analysis\n  ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/breast-cancer-dataset/breast-cancer.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.235382Z","iopub.execute_input":"2023-02-04T11:18:48.23612Z","iopub.status.idle":"2023-02-04T11:18:48.274761Z","shell.execute_reply.started":"2023-02-04T11:18:48.236078Z","shell.execute_reply":"2023-02-04T11:18:48.27336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df, x='diagnosis', color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])\n","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.27677Z","iopub.execute_input":"2023-02-04T11:18:48.277133Z","iopub.status.idle":"2023-02-04T11:18:48.353965Z","shell.execute_reply.started":"2023-02-04T11:18:48.2771Z","shell.execute_reply":"2023-02-04T11:18:48.35265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df,x='area_mean',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.356378Z","iopub.execute_input":"2023-02-04T11:18:48.356845Z","iopub.status.idle":"2023-02-04T11:18:48.42969Z","shell.execute_reply.started":"2023-02-04T11:18:48.356807Z","shell.execute_reply":"2023-02-04T11:18:48.428509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df,x='radius_mean',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.43106Z","iopub.execute_input":"2023-02-04T11:18:48.431442Z","iopub.status.idle":"2023-02-04T11:18:48.50972Z","shell.execute_reply.started":"2023-02-04T11:18:48.431408Z","shell.execute_reply":"2023-02-04T11:18:48.508147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df,x='perimeter_mean',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.512849Z","iopub.execute_input":"2023-02-04T11:18:48.513258Z","iopub.status.idle":"2023-02-04T11:18:48.587222Z","shell.execute_reply.started":"2023-02-04T11:18:48.513223Z","shell.execute_reply":"2023-02-04T11:18:48.586066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df,x='smoothness_mean',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.588609Z","iopub.execute_input":"2023-02-04T11:18:48.588962Z","iopub.status.idle":"2023-02-04T11:18:48.662693Z","shell.execute_reply.started":"2023-02-04T11:18:48.588929Z","shell.execute_reply":"2023-02-04T11:18:48.661105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df,x='texture_mean',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.665108Z","iopub.execute_input":"2023-02-04T11:18:48.665619Z","iopub.status.idle":"2023-02-04T11:18:48.741962Z","shell.execute_reply.started":"2023-02-04T11:18:48.665572Z","shell.execute_reply":"2023-02-04T11:18:48.740275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.scatter(data_frame=df,x='symmetry_worst',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])\n","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.743777Z","iopub.execute_input":"2023-02-04T11:18:48.744223Z","iopub.status.idle":"2023-02-04T11:18:48.817151Z","shell.execute_reply.started":"2023-02-04T11:18:48.744163Z","shell.execute_reply":"2023-02-04T11:18:48.815832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.scatter(data_frame=df,x='concavity_worst',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])\n","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.819394Z","iopub.execute_input":"2023-02-04T11:18:48.819869Z","iopub.status.idle":"2023-02-04T11:18:48.893017Z","shell.execute_reply.started":"2023-02-04T11:18:48.819832Z","shell.execute_reply":"2023-02-04T11:18:48.89139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.scatter(data_frame=df,x='fractal_dimension_worst',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])\n","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.896034Z","iopub.execute_input":"2023-02-04T11:18:48.896438Z","iopub.status.idle":"2023-02-04T11:18:48.966016Z","shell.execute_reply.started":"2023-02-04T11:18:48.896404Z","shell.execute_reply":"2023-02-04T11:18:48.964583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h1 style='background:#000000;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #00F1FF;'>Data Preprocessing</center></h1>\n\n# Data Preprocessing\n  ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/breast-cancer-dataset/breast-cancer.csv')\n                 \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:48.967709Z","iopub.execute_input":"2023-02-04T11:18:48.968062Z","iopub.status.idle":"2023-02-04T11:18:49.00707Z","shell.execute_reply.started":"2023-02-04T11:18:48.968031Z","shell.execute_reply":"2023-02-04T11:18:49.005663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('id', axis=1, inplace=True) #drop redundant columns","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:49.008826Z","iopub.execute_input":"2023-02-04T11:18:49.009259Z","iopub.status.idle":"2023-02-04T11:18:49.017627Z","shell.execute_reply.started":"2023-02-04T11:18:49.009219Z","shell.execute_reply":"2023-02-04T11:18:49.016157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:49.019509Z","iopub.execute_input":"2023-02-04T11:18:49.021349Z","iopub.status.idle":"2023-02-04T11:18:49.128904Z","shell.execute_reply.started":"2023-02-04T11:18:49.02125Z","shell.execute_reply":"2023-02-04T11:18:49.127524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encode the target","metadata":{}},{"cell_type":"code","source":"df['diagnosis'] = (df['diagnosis'] == 'M').astype(int) #encode the label into 1/0","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:49.131014Z","iopub.execute_input":"2023-02-04T11:18:49.131534Z","iopub.status.idle":"2023-02-04T11:18:49.139809Z","shell.execute_reply.started":"2023-02-04T11:18:49.131485Z","shell.execute_reply":"2023-02-04T11:18:49.138196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get highly correlated features","metadata":{}},{"cell_type":"code","source":"corr = df.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cmap='mako_r',annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:50.107495Z","iopub.execute_input":"2023-02-04T11:18:50.107911Z","iopub.status.idle":"2023-02-04T11:18:54.163265Z","shell.execute_reply.started":"2023-02-04T11:18:50.107867Z","shell.execute_reply":"2023-02-04T11:18:54.162047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the absolute value of the correlation\ncor_target = abs(corr[\"diagnosis\"])\n\n# Select highly correlated features (thresold = 0.2)\nrelevant_features = cor_target[cor_target>0.2]\n\n# Collect the names of the features\nnames = [index for index, value in relevant_features.iteritems()]\n\n# Drop the target variable from the results\nnames.remove('diagnosis')\n\n# Display the results\nprint(names)","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:54.165436Z","iopub.execute_input":"2023-02-04T11:18:54.166719Z","iopub.status.idle":"2023-02-04T11:18:54.178043Z","shell.execute_reply.started":"2023-02-04T11:18:54.166664Z","shell.execute_reply":"2023-02-04T11:18:54.17669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Assign data and labels","metadata":{}},{"cell_type":"code","source":"X = df[names].values\ny = df['diagnosis'].values","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:54.180137Z","iopub.execute_input":"2023-02-04T11:18:54.181274Z","iopub.status.idle":"2023-02-04T11:18:54.193978Z","shell.execute_reply.started":"2023-02-04T11:18:54.181163Z","shell.execute_reply":"2023-02-04T11:18:54.192611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split(X, y, random_state=41, test_size=0.2):\n    \"\"\"\n    Splits the data into training and testing sets.\n\n    Parameters:\n        X (numpy.ndarray): Features array of shape (n_samples, n_features).\n        y (numpy.ndarray): Target array of shape (n_samples,).\n        random_state (int): Seed for the random number generator. Default is 42.\n        test_size (float): Proportion of samples to include in the test set. Default is 0.2.\n\n    Returns:\n        Tuple[numpy.ndarray]: A tuple containing X_train, X_test, y_train, y_test.\n    \"\"\"\n    # Get number of samples\n    n_samples = X.shape[0]\n\n    # Set the seed for the random number generator\n    np.random.seed(random_state)\n\n    # Shuffle the indices\n    shuffled_indices = np.random.permutation(np.arange(n_samples))\n\n    # Determine the size of the test set\n    test_size = int(n_samples * test_size)\n\n    # Split the indices into test and train\n    test_indices = shuffled_indices[:test_size]\n    train_indices = shuffled_indices[test_size:]\n\n    # Split the features and target arrays into test and train\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n\n    return X_train, X_test, y_train, y_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scale(X):\n    \"\"\"\n    Standardizes the data in the array X.\n\n    Parameters:\n        X (numpy.ndarray): Features array of shape (n_samples, n_features).\n\n    Returns:\n        numpy.ndarray: The standardized features array.\n    \"\"\"\n    # Calculate the mean and standard deviation of each feature\n    mean = np.mean(X, axis=0)\n    std = np.std(X, axis=0)\n\n    # Standardize the data\n    X = (X - mean) / std\n    return X\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = scale(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42) #split the  data into traing and validating\n","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:54.197179Z","iopub.execute_input":"2023-02-04T11:18:54.198603Z","iopub.status.idle":"2023-02-04T11:18:54.207988Z","shell.execute_reply.started":"2023-02-04T11:18:54.198557Z","shell.execute_reply":"2023-02-04T11:18:54.206474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h1 style='background:#000000;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #00F1FF;'>Model Implementation Helper Functions</center></h1>\n\n# Model Implementation Helper Functions\n  ","metadata":{}},{"cell_type":"markdown","source":"## Activation Functions\n","metadata":{}},{"cell_type":"markdown","source":"**The Rectified Linear Unit (ReLU) is a simple, yet highly effective activation function commonly used in Neural Networks. It is defined as:**\n\n**\\begin{equation}\nf(Z) = max(0, Z)\n\\end{equation}**\n\n**where $Z$ is the input to the function.**\n\n**ReLU sets all negative values of $Z$ to zero, and leaves the positive values unchanged. This non-linear activation function helps Neural Networks model complex non-linear relationships between inputs and outputs, allowing them to learn more complex representations of the data.**\n\n**In addition to its effectiveness in Neural Networks, ReLU is also computationally efficient and easy to implement.**","metadata":{}},{"cell_type":"code","source":"def relu(Z):\n    \"\"\"\n    Implement the ReLU function.\n\n    Arguments:\n    Z -- Output of the linear layer\n\n    Returns:\n    A -- Post-activation parameter\n    cache -- used for backpropagation\n    \"\"\"\n    A = np.maximum(0,Z)\n    cache = Z \n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:54.227851Z","iopub.execute_input":"2023-02-04T11:18:54.228274Z","iopub.status.idle":"2023-02-04T11:18:54.236016Z","shell.execute_reply.started":"2023-02-04T11:18:54.228234Z","shell.execute_reply":"2023-02-04T11:18:54.234739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = np.linspace(-12, 12, 200)\nfig = px.line(x=z, y=relu(z)[0],title='ReLU Function',template=\"plotly_dark\")\nfig.update_layout(\n    title_font_color=\"#00F1FF\", \n    xaxis=dict(color=\"#00F1FF\"), \n    yaxis=dict(color=\"#00F1FF\") \n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:54.238102Z","iopub.execute_input":"2023-02-04T11:18:54.238767Z","iopub.status.idle":"2023-02-04T11:18:54.343127Z","shell.execute_reply.started":"2023-02-04T11:18:54.23873Z","shell.execute_reply":"2023-02-04T11:18:54.341553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The derivative of the ReLU function can be computed as:\n\n**\\begin{equation}\nf'(Z) = \\begin{cases}\n0, & \\text{if } Z \\leq 0 \\\n1, & \\text{if } Z > 0\n\\end{cases}\n\\end{equation}**","metadata":{}},{"cell_type":"code","source":"def relu_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single ReLU unit.\n\n    Arguments:\n    dA -- post-activation gradient\n    cache -- 'Z'  stored for backpropagation\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    dZ = np.array(dA, copy=True) \n    # When z <= 0, dz is equal to 0 as well. \n    dZ[Z <= 0] = 0\n    \n    return dZ","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:54.344891Z","iopub.execute_input":"2023-02-04T11:18:54.34528Z","iopub.status.idle":"2023-02-04T11:18:54.352186Z","shell.execute_reply.started":"2023-02-04T11:18:54.345246Z","shell.execute_reply":"2023-02-04T11:18:54.350898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sigmoid\n\n**The Sigmoid function is a common activation function used in Neural Networks, particularly for binary classification problems. It is represented by the following formula:**\n\n**\\begin{equation}\nf(Z) = \\frac{1}{1+e^{-Z}}\n\\end{equation}**\n\n**where $Z$ is the input to the function.**\n\n**The Sigmoid function maps any real-valued number to a value between 0 and 1, which can be interpreted as a probability. In binary classification problems, we often use the Sigmoid function as the activation function for the output layer of the Neural Network, since it can be used to compute the probability of the input belonging to the positive class.**\n\n\n\n","metadata":{}},{"cell_type":"code","source":"def sigmoid(Z):\n    \"\"\"\n    Implement the Sigmoid function.\n\n    Arguments:\n    Z -- Output of the linear layer\n\n    Returns:\n    A -- Post-activation parameter\n    cache -- a python dictionary containing \"A\" for backpropagation\n    \"\"\"\n    A = 1/(1+np.exp(-Z))\n    cache = Z\n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:54.353552Z","iopub.execute_input":"2023-02-04T11:18:54.354895Z","iopub.status.idle":"2023-02-04T11:18:54.372236Z","shell.execute_reply.started":"2023-02-04T11:18:54.354847Z","shell.execute_reply":"2023-02-04T11:18:54.370433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = np.linspace(-12, 12, 200)\nfig = px.line(x=z, y=sigmoid(z)[0],title='Sigmoid Function',template=\"plotly_dark\")\nfig.update_layout(\n    title_font_color=\"#00F1FF\", \n    xaxis=dict(color=\"#00F1FF\"), \n    yaxis=dict(color=\"#00F1FF\") \n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:54.375429Z","iopub.execute_input":"2023-02-04T11:18:54.377042Z","iopub.status.idle":"2023-02-04T11:18:54.445419Z","shell.execute_reply.started":"2023-02-04T11:18:54.376981Z","shell.execute_reply":"2023-02-04T11:18:54.444215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The derivative of the Sigmoid function can be computed as:\n\n\\begin{equation}\nf'(Z) = f(Z)(1-f(Z))\n\\end{equation}","metadata":{}},{"cell_type":"code","source":"def sigmoid_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single sigmoid unit.\n\n    Arguments:\n    dA -- post-activation gradient\n    cache -- 'Z' stored during forward pass\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    s = 1/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    return dZ","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:18:54.447511Z","iopub.execute_input":"2023-02-04T11:18:54.447912Z","iopub.status.idle":"2023-02-04T11:18:54.454169Z","shell.execute_reply.started":"2023-02-04T11:18:54.447874Z","shell.execute_reply":"2023-02-04T11:18:54.453021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<h1 style='background:#000000;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #00F1FF;'>Model Implementation</center></h1>\n\n# Model Implementation\n  ","metadata":{}},{"cell_type":"markdown","source":"**As always we'll be using [vectorized implementation](https://www.kaggle.com/code/fareselmenshawii/vectorization/edit/run/109247974)**","metadata":{}},{"cell_type":"markdown","source":"# How the Algorithm works\n\n**Initialize Parameters: We start by initializing the weights and biases of the model. For each layer $l$ in the network, we initialize $W^{[l]}$ to be a matrix with dimensions $(n^{[l]}, n^{[l-1]})$, where $n^{[l]}$ is the number of units in layer $l$ and $n^{[l-1]}$ is the number of units in the previous layer. We also initialize $b^{[l]}$ to be a vector with dimensions $(n^{[l]}, 1)$.**\n\n**Forward Propagation: In the forward pass, we propagate through the network calculating the output of every layer. For each layer $l$ in the network, we calculate:**\n\n**\\begin{equation}\nZ^{[l]} = W^{[l]}.A^{[l-1]} +b^{[l]}\n\\end{equation}**\n\n**\\begin{equation}\nA^{[l]} = g(Z^{[l]})\n\\end{equation}**\n\n**where $A^{[0]} = X$ is the input to the network and $g(.)$ is the activation function used in layer $l$**.\n\n**Compute Cost: We calculate the cost function to determine how well we are doing. For binary classification problems, we often use the binary cross-entropy to measure our network performance. The cost function can be computed as:**\n\n**\\begin{equation}\nJ = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[y^{(i)} \\log\\left(a^{L}\\right) + (1-y^{(i)})\\log\\left(1- a^{L}\\right)\\right]\n\\end{equation}**\n\n**where $a^{L}$ is the predicted output of the network for the $i$-th input example and $y^{(i)}$ is the true output for the $i$-th input example.**\n\n**Backpropagation: In the backward pass, we compute the derivatives of the loss function with respect to the parameters of the network using the chain rule of differentiation. Specifically, we calculate the derivatives of the cost function with respect to $Z^{[l]}$, which can then be used to calculate the derivatives of the cost function with respect to $W^{[l]}$ and $b^{[l]}$.**\n\n**Updating Parameters: We update the parameters of the network using gradient descent, which tries to reduce the cost by adjusting the parameters in the opposite direction of the gradient. The gradient descent rule is, for each layer $l$ in the network:**\n\n**\\begin{equation}\nW^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}\n\\end{equation}**\n\n**\\begin{equation}\nb^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}\n\\end{equation}**\n\n**where $\\alpha$ is the learning rate and $dW^{[l]}$ and $db^{[l]}$ are the derivatives of the cost function with respect to $W^{[l]}$ and $b^{[l]}$, respectively.**\n","metadata":{}},{"cell_type":"code","source":"class NeuralNetwork:\n    def __init__(self, layer_dimensions=[25,16,16,1],learning_rate=0.00001):\n        \"\"\"\n        Parameters\n        ----------\n\n        layer_dimensions : list\n            python array (list) containing the dimensions of each layer in our network\n                \n        learning_rate :  float\n            learning rate of the network.\n\n        \"\"\"\n        self.layer_dimensions = layer_dimensions\n        self.learning_rate = learning_rate\n        \n        \n    def initialize_parameters(self):\n        \"\"\"initializes the parameters\"\"\"\n        np.random.seed(3)\n        self.n_layers =  len(self.layer_dimensions)\n        for l in range(1, self.n_layers):\n            vars(self)[f'W{l}'] = np.random.randn(self.layer_dimensions[l], self.layer_dimensions[l-1]) * 0.01\n            vars(self)[f'b{l}'] = np.zeros((self.layer_dimensions[l], 1))\n\n    \n    def _linear_forward(self, A, W, b):\n        \"\"\"\n        Implements the linear part of a layer's forward propagation.\n\n        Arguments:\n        A -- activations from previous layer (size of previous layer, number of examples)\n        W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n        b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n        Returns:\n        Z -- pre-activation parameter \n        cache -- a python tuple containing \"A\", \"W\" and \"b\"  for backpropagation\n        \"\"\"\n        # Compute Z\n        Z = np.dot(W,A) + b\n        # Cache  A, W , b for backpropagation\n        cache = (A, W, b)\n        return Z, cache\n    \n    def _forward_propagation(self,A_prev ,W ,b , activation):\n        \"\"\"\n        Implements the forward propagation for a network layer\n\n        Arguments:\n        A_prev -- activations from previous layer, shape : (size of previous layer, number of examples)\n        W -- shape : (size of current layer, size of previous layer)\n        b -- shape : (size of the current layer, 1)\n        activation -- the activation to be used in this layer\n\n        Returns:\n        A -- the output of the activation function \n        cache -- a python tuple containing \"linear_cache\" and \"activation_cache\" for backpropagation\n        \"\"\"\n        \n        # Compute Z using the function defined above, compute A using the activaiton function\n        if activation == \"sigmoid\":\n            Z, linear_cache = self._linear_forward(A_prev, W, b)\n            A, activation_cache = sigmoid(Z) \n        elif activation == \"relu\":\n            Z, linear_cache = self._linear_forward(A_prev, W, b) \n            A, activation_cache = relu(Z) \n            #Store the cache for backpropagation\n        cache = (linear_cache, activation_cache)\n        return A, cache\n    \n    \n    def forward_propagation(self, X):\n        \"\"\"\n        Implements forward propagation for the whole network\n\n        Arguments:\n        X --  shape : (input size, number of examples)\n\n        Returns:\n        AL -- last post-activation value\n        caches -- list of cache returned by _forward_propagation helper function\n        \"\"\"\n        # Initialize empty list to store caches\n        caches = []\n        # Set initial A to X \n        A = X\n        L =  self.n_layers -1\n        for l in range(1, L):\n            A_prev = A \n            # Forward propagate through the network except the last layer\n            A, cache = self._forward_propagation(A_prev, vars(self)['W' + str(l)], vars(self)['b' + str(l)], \"relu\")\n            caches.append(cache)\n        # Forward propagate through the output layer and get the predictions\n        predictions, cache = self._forward_propagation(A, vars(self)['W' + str(L)], vars(self)['b' + str(L)], \"sigmoid\")\n        # Append the cache to caches list recall that cache will be (linear_cache, activation_cache)\n        caches.append(cache)\n\n        return predictions, caches\n    \n    def compute_cost(self, predictions, y):\n        \"\"\"\n        Implements the cost function \n\n        Arguments:\n        predictions -- The model predictions, shape : (1, number of examples)\n        y -- The true values, shape : (1, number of examples)\n\n        Returns:\n        cost -- cross-entropy cost\n        \"\"\"\n        # Get number of training examples\n        m = y.shape[0]\n        # Compute cost we're adding small epsilon for numeric stability\n        cost = (-1/m) * (np.dot(y, np.log(predictions+1e-9).T) + np.dot((1-y), np.log(1-predictions+1e-9).T))\n        # squeeze the cost to set it into the correct shape \n        cost = np.squeeze(cost)\n        return cost   \n        \n    def _linear_backward(self, dZ, cache):\n        \"\"\"\n        Implements the linear portion of backward propagation \n\n        Arguments:\n        dZ -- Gradient of the cost with respect to the linear output of the current layer \n        cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n        Returns:\n        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n        dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n        db -- Gradient of the cost with respect to b (current layer l), same shape as b\n        \"\"\"\n        # Get the cache from forward propagation\n        A_prev, W, b = cache\n        # Get number of training examples\n        m = A_prev.shape[1]\n        # Compute gradients for W, b and A\n        dW = (1/m) * np.dot(dZ, A_prev.T)\n        db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n        dA_prev = np.dot(W.T,dZ)\n        return dA_prev, dW, db\n    \n            \n    def _back_propagation(self, dA, cache, activation):\n        \"\"\"\n        Implements the backward propagation for a single layer.\n\n        Arguments:\n        dA -- post-activation gradient for current layer l \n        cache -- tuple of values (linear_cache, activation_cache) \n        activation -- the activation to be used in this layer\n\n        Returns:\n        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n        dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n        db -- Gradient of the cost with respect to b (current layer l), same shape as b\n        \"\"\"\n        # get the cache from forward propagation and activation derivates function\n        linear_cache, activation_cache = cache\n        # compute gradients for Z depending on the activation function\n        if activation == \"relu\":\n            dZ = relu_backward(dA, activation_cache)\n\n        elif activation == \"sigmoid\":\n            dZ = sigmoid_backward(dA, activation_cache)\n        # Compute gradients for W, b and A \n        dA_prev, dW, db = self._linear_backward(dZ, linear_cache)\n        return dA_prev, dW, db\n\n    def back_propagation(self, predictions, Y, caches):\n        \"\"\"\n        Implements the backward propagation for the NeuralNetwork\n\n        Arguments:\n        Prediction --  output of the forward propagation \n        Y -- true label\n        caches -- list of caches\n        \"\"\"\n        L =  self.n_layers - 1\n        # Get number of examples\n        m = predictions.shape[1]\n        Y = Y.reshape(predictions.shape) \n        # Initializing the backpropagation we're adding a small epsilon for numeric stability \n        dAL = - (np.divide(Y, predictions+1e-9) - np.divide(1 - Y, 1 - predictions+1e-9))\n        current_cache = caches[L-1] # Last Layer\n        # Compute gradients of the predictions\n        vars(self)[f'dA{L-1}'], vars(self)[f'dW{L}'], vars(self)[f'db{L}'] = self._back_propagation(dAL, current_cache, \"sigmoid\")\n        for l in reversed(range(L-1)):\n            # update the cache\n            current_cache = caches[l]\n            # compute gradients of the network layers \n            vars(self)[f'dA{l}'] , vars(self)[f'dW{l+1}'], vars(self)[f'db{l+1}'] = self._back_propagation(vars(self)[f'dA{l + 1}'], current_cache, activation = \"relu\")\n            \n\n\n    def update_parameters(self):\n            \"\"\"\n            Updates parameters using gradient descent\n            \"\"\"\n            L = self.n_layers - 1\n            # Loop over parameters and update them using computed gradients\n            for l in range(L):\n                vars(self)[f'W{l+1}'] = vars(self)[f'W{l+1}'] - self.learning_rate * vars(self)[f'dW{l+1}']\n                vars(self)[f'b{l+1}']  = vars(self)[f'b{l+1}'] - self.learning_rate * vars(self)[f'db{l+1}']\n                \n\n    def fit(self,X, Y, epochs=2000, print_cost=True):\n            \"\"\"\n            Trains the Neural Network using input data\n            \n            Arguments:\n            X -- input data\n            Y -- true \"label\" \n            Epochs -- number of iterations of the optimization loop\n            print_cost -- If set to True, this will print the cost every 100 iterations \n            \"\"\"\n            # Transpose X to get the correct shape\n            X = X.T\n            np.random.seed(1)\n            #create empty array to store the costs\n            costs = [] \n            # Get number of training examples\n            m = X.shape[1]                           \n            # Initialize parameters \n            self.initialize_parameters()\n            # loop for stated number of epochs\n            for i in range(0, epochs):\n                # Forward propagate and get the predictions and caches\n                predictions, caches = self.forward_propagation(X)\n                #compute the cost function\n                cost = self.compute_cost(predictions, Y)\n                # Calculate the gradient and update the parameters\n                self.back_propagation(predictions, Y, caches)\n\n                self.update_parameters()\n\n\n                # Print the cost every 10000 training example\n                if print_cost and i % 5000 == 0:\n                    print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n                if print_cost and i % 5000 == 0:\n                    costs.append(cost)\n            if print_cost:         \n            # Plot the cost over training    \n                fig = px.line(y=np.squeeze(costs),title='Cost',template=\"plotly_dark\")\n                fig.update_layout(\n                    title_font_color=\"#00F1FF\", \n                    xaxis=dict(color=\"#00F1FF\"), \n                    yaxis=dict(color=\"#00F1FF\") \n                )\n                fig.show()\n\n\n    def predict(self,X,y):\n        \"\"\"\n        uses the trained model to predict given X value\n\n        Arguments:\n        X -- data set of examples you would like to label\n        y -- True values of examples; used for measuring the model's accuracy\n        Returns:\n        predictions -- predictions for the given dataset X\n        \"\"\"\n        X = X.T\n        # Get predictions from forward propagation\n        predictions, _ = self.forward_propagation(X)\n        # Predictions Above 0.5 are True otherwise they are False\n        predictions = (predictions > 0.5)\n        # Squeeze the predictions into the correct shape and cast true/false values to 1/0\n        predictions = np.squeeze(predictions.astype(int))\n        #Print the accuracy\n        return np.sum((predictions == y)/X.shape[1]), predictions.T","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:20:15.281022Z","iopub.execute_input":"2023-02-04T11:20:15.281493Z","iopub.status.idle":"2023-02-04T11:20:15.322082Z","shell.execute_reply.started":"2023-02-04T11:20:15.281456Z","shell.execute_reply":"2023-02-04T11:20:15.320442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<h1 style='background:#000000;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #00F1FF;'>Evaluation</center></h1>\n\n# Evaluation\n  ","metadata":{}},{"cell_type":"markdown","source":"## Create evaluation function","metadata":{}},{"cell_type":"code","source":"def train_evaluate_model(X_train, y_train, X_test, y_test, learning_rate, layer_dimensions, epochs):\n    '''\n    Keyword arguments:\n    X_train -- Training data\n    y_train -- Traing labels\n    X_train -- test data\n    y_train -- test labels\n    layer_dimensions -- python array (list) containing the dimensions of each layer in our network\n    learning_rate --  learning rate of the network.\n    Epochs -- number of iterations of the optimization loop\n    returns a dataframe \n    '''\n    # create model instance with the given hyperparameters\n    model = NeuralNetwork(learning_rate=learning_rate,layer_dimensions=layers)\n    # fit the model\n    model.fit(X_train, y_train,epochs=epochs,print_cost=False)\n    accuracy, predictions = model.predict(X_test, y_test) # calculate accuracy and predictions\n    \n    #create a dataframe to visualize the results\n    eval_df = pd.DataFrame([[learning_rate, layer_dimensions, epochs, accuracy]], columns=['Learning_Rate', 'Layers', 'Epochs', 'Accuracy'])\n    return eval_df","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:20:42.382499Z","iopub.execute_input":"2023-02-04T11:20:42.38291Z","iopub.status.idle":"2023-02-04T11:20:42.391927Z","shell.execute_reply.started":"2023-02-04T11:20:42.382877Z","shell.execute_reply":"2023-02-04T11:20:42.390178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\nlayers = [25,1,1]\nepochs = 3000\nresults = train_evaluate_model(X_train, y_train, X_test, y_test, learning_rate=learning_rate, layer_dimensions=layers, epochs=epochs)","metadata":{"execution":{"iopub.status.busy":"2023-02-04T11:20:49.755372Z","iopub.execute_input":"2023-02-04T11:20:49.75581Z","iopub.status.idle":"2023-02-04T11:20:51.612029Z","shell.execute_reply.started":"2023-02-04T11:20:49.755775Z","shell.execute_reply":"2023-02-04T11:20:51.610222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.index = ['Model_1']","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:15.603454Z","iopub.execute_input":"2022-10-28T19:55:15.604346Z","iopub.status.idle":"2022-10-28T19:55:15.610565Z","shell.execute_reply.started":"2022-10-28T19:55:15.604283Z","shell.execute_reply":"2022-10-28T19:55:15.609137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.style.background_gradient(cmap =sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:16.034932Z","iopub.execute_input":"2022-10-28T19:55:16.035312Z","iopub.status.idle":"2022-10-28T19:55:16.053539Z","shell.execute_reply.started":"2022-10-28T19:55:16.035282Z","shell.execute_reply":"2022-10-28T19:55:16.052305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\nlayers = [25,16,1]\nepochs = 3000\ntemp_df = train_evaluate_model(X_train, y_train, X_test, y_test, learning_rate=learning_rate, layer_dimensions=layers, epochs=epochs)\ntemp_df.index = ['Model_2']\nresults = results.append(temp_df)","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:16.9749Z","iopub.execute_input":"2022-10-28T19:55:16.975788Z","iopub.status.idle":"2022-10-28T19:55:17.9516Z","shell.execute_reply.started":"2022-10-28T19:55:16.975747Z","shell.execute_reply":"2022-10-28T19:55:17.950706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.style.background_gradient(cmap =sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:19.735373Z","iopub.execute_input":"2022-10-28T19:55:19.73575Z","iopub.status.idle":"2022-10-28T19:55:19.753508Z","shell.execute_reply.started":"2022-10-28T19:55:19.73572Z","shell.execute_reply":"2022-10-28T19:55:19.752443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0001\nlayers = [25,16,1]\nepochs = 3000\ntemp_df = train_evaluate_model(X_train, y_train, X_test, y_test, learning_rate=learning_rate, layer_dimensions=layers, epochs=epochs)\ntemp_df.index = ['Model_3']\nresults = results.append(temp_df)","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:21.428285Z","iopub.execute_input":"2022-10-28T19:55:21.42869Z","iopub.status.idle":"2022-10-28T19:55:22.337752Z","shell.execute_reply.started":"2022-10-28T19:55:21.428657Z","shell.execute_reply":"2022-10-28T19:55:22.336595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.style.background_gradient(cmap =sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:23.165576Z","iopub.execute_input":"2022-10-28T19:55:23.166716Z","iopub.status.idle":"2022-10-28T19:55:23.183755Z","shell.execute_reply.started":"2022-10-28T19:55:23.166674Z","shell.execute_reply":"2022-10-28T19:55:23.182875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0001\nlayers = [25,16,1]\nepochs = 30000\ntemp_df = train_evaluate_model(X_train, y_train, X_test, y_test, learning_rate=learning_rate, layer_dimensions=layers, epochs=epochs)\ntemp_df.index = ['Model_4']\nresults = results.append(temp_df)","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:24.064804Z","iopub.execute_input":"2022-10-28T19:55:24.065192Z","iopub.status.idle":"2022-10-28T19:55:33.087871Z","shell.execute_reply.started":"2022-10-28T19:55:24.065161Z","shell.execute_reply":"2022-10-28T19:55:33.086904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.style.background_gradient(cmap =sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:33.089547Z","iopub.execute_input":"2022-10-28T19:55:33.08984Z","iopub.status.idle":"2022-10-28T19:55:33.107256Z","shell.execute_reply.started":"2022-10-28T19:55:33.089814Z","shell.execute_reply":"2022-10-28T19:55:33.106516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0001\nlayers = [25,16,16,1]\nepochs = 30000\ntemp_df = train_evaluate_model(X_train, y_train, X_test, y_test, learning_rate=learning_rate, layer_dimensions=layers, epochs=epochs)\ntemp_df.index = ['Model_5']\nresults = results.append(temp_df)","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:33.108514Z","iopub.execute_input":"2022-10-28T19:55:33.10903Z","iopub.status.idle":"2022-10-28T19:55:46.497739Z","shell.execute_reply.started":"2022-10-28T19:55:33.108999Z","shell.execute_reply":"2022-10-28T19:55:46.496579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.style.background_gradient(cmap =sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:46.500912Z","iopub.execute_input":"2022-10-28T19:55:46.501269Z","iopub.status.idle":"2022-10-28T19:55:46.520801Z","shell.execute_reply.started":"2022-10-28T19:55:46.501238Z","shell.execute_reply":"2022-10-28T19:55:46.51998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0001\nlayers = [25,16,16,16,1]\nepochs = 30000\ntemp_df = train_evaluate_model(X_train, y_train, X_test, y_test, learning_rate=learning_rate, layer_dimensions=layers, epochs=epochs)\ntemp_df.index = ['Model_6']\nresults = results.append(temp_df)","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:55:46.522007Z","iopub.execute_input":"2022-10-28T19:55:46.522517Z","iopub.status.idle":"2022-10-28T19:56:03.778826Z","shell.execute_reply.started":"2022-10-28T19:55:46.522484Z","shell.execute_reply":"2022-10-28T19:56:03.777769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.style.background_gradient(cmap =sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:56:03.780452Z","iopub.execute_input":"2022-10-28T19:56:03.781074Z","iopub.status.idle":"2022-10-28T19:56:03.798933Z","shell.execute_reply.started":"2022-10-28T19:56:03.781026Z","shell.execute_reply":"2022-10-28T19:56:03.797652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0001\nlayers = [25,32,32,1]\nepochs = 30000\ntemp_df = train_evaluate_model(X_train, y_train, X_test, y_test, learning_rate=learning_rate, layer_dimensions=layers, epochs=epochs)\ntemp_df.index = ['Model_7']\nresults = results.append(temp_df)","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:56:03.800566Z","iopub.execute_input":"2022-10-28T19:56:03.801487Z","iopub.status.idle":"2022-10-28T19:56:46.350937Z","shell.execute_reply.started":"2022-10-28T19:56:03.801444Z","shell.execute_reply":"2022-10-28T19:56:46.349305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.style.background_gradient(cmap =sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:56:46.358754Z","iopub.execute_input":"2022-10-28T19:56:46.362758Z","iopub.status.idle":"2022-10-28T19:56:46.403189Z","shell.execute_reply.started":"2022-10-28T19:56:46.362685Z","shell.execute_reply":"2022-10-28T19:56:46.401691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0001\nlayers = [25,128,128,1]\nepochs = 30000\ntemp_df = train_evaluate_model(X_train, y_train, X_test, y_test, learning_rate=learning_rate, layer_dimensions=layers, epochs=epochs)\ntemp_df.index = ['Model_8']\nresults = results.append(temp_df)","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:56:46.410431Z","iopub.execute_input":"2022-10-28T19:56:46.414496Z","iopub.status.idle":"2022-10-28T19:58:53.129622Z","shell.execute_reply.started":"2022-10-28T19:56:46.414407Z","shell.execute_reply":"2022-10-28T19:58:53.128132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.style.background_gradient(cmap =sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:58:53.140066Z","iopub.execute_input":"2022-10-28T19:58:53.14412Z","iopub.status.idle":"2022-10-28T19:58:53.184096Z","shell.execute_reply.started":"2022-10-28T19:58:53.144048Z","shell.execute_reply":"2022-10-28T19:58:53.182663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## View training process for the best results \"Model 5 with lowest runtime and highest accuracy\"","metadata":{}},{"cell_type":"code","source":"model = NeuralNetwork(learning_rate=0.0001)\n\nmodel.fit(X_train, y_train,epochs=30000,print_cost=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:58:53.190636Z","iopub.execute_input":"2022-10-28T19:58:53.194351Z","iopub.status.idle":"2022-10-28T19:59:06.56734Z","shell.execute_reply.started":"2022-10-28T19:58:53.194291Z","shell.execute_reply":"2022-10-28T19:59:06.566365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy,predictions = model.predict(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-28T19:59:06.568919Z","iopub.execute_input":"2022-10-28T19:59:06.569497Z","iopub.status.idle":"2022-10-28T19:59:06.574809Z","shell.execute_reply.started":"2022-10-28T19:59:06.569456Z","shell.execute_reply":"2022-10-28T19:59:06.573948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n<h1 style='background:#000000;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #00F1FF;'>Thank You</center></h1>\n\n  \n# Thank you\n**Thank you for going through this notebook**\n\n**If you have any suggestions please let me know**","metadata":{}},{"cell_type":"markdown","source":"<div style=\"padding:10px; \n            color:#333333;\n            margin:10px;\n            font-size:150%;\n            display:fill;\n            border-radius:1px;\n            border-style:solid;\n            border-color:#666666;\n            background-color:#F9F9F9;\n            overflow:hidden;\">\n    <center>\n        <a id='top'></a>\n        <b>Machine Learning From Scratch Series</b>\n    </center>\n    <br>\n    <ul>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/linear-regression-from-scratch\" style=\"color:#0072B2\">1 - Linear Regression</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch\" style=\"color:#0072B2\">2 -  Logistic Regression</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/kmeans-from-scratch\" style=\"color:#0072B2\">3 - KMeans</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/decision-tree-classifier-from-scratch\" style=\"color:#0072B2\">4 - Decision Trees</a>\n        </li> \n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/random-forest-classifier-from-scratch\" style=\"color:#0072B2\">5 -  Random Forest</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/knn-from-scratch\" style=\"color:#0072B2\">6 - KNearestNeighbor</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/pca-from-scratch?scriptVersionId=121402593\" style=\"color:#0072B2\">7 - PCA</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/svm-from-scratch\" style=\"color:#0072B2\">8 - SVM</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/naive-bayes-from-scratch\" style=\"color:#0072B2\">9 - Naive Baye</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/optimized-neural-network-from-scratch\" style=\"color:#0072B2\">10 - Optimized Neural Network</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/neural-network-from-scratch\" style=\"color:#0072B2\">11 - Neural Network</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/cnn-from-scratch\" style=\"color:#0072B2\">12 - CNN</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch\" style=\"color:#0072B2\">13 - RNN</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/lstm-from-scratch\" style=\"color:#0072B2\">14 - LSTM</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/gru-from-scratch\" style=\"color:#0072B2\">15 - GRU</a>\n        </li>\n    </ul>\n</div>","metadata":{}}]}